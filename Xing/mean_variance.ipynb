{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "d1a42579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from arch.univariate import ConstantMean, GARCH, StudentsT, Normal\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # nuke all warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "3d4177e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_enriched.csv')\n",
    "X_val = pd.read_csv('X_val_enriched.csv')\n",
    "y_train = pd.read_csv('y_train.csv')['market_forward_excess_returns']\n",
    "y_val = pd.read_csv('y_val.csv')['market_forward_excess_returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "cceb0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_train = X_train[-252:]\n",
    "X_train = X_train[:-252]\n",
    "y_features_train = y_train[-252:]\n",
    "y_train = y_train[:-252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "b1031c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = X_train.ffill().bfill()\n",
    "X_features_train_cleaned = X_features_train.ffill().bfill()\n",
    "X_val_cleaned = X_val.ffill().bfill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "a335b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_na_train = X_train_cleaned.columns[X_train_cleaned.isna().any()]\n",
    "cols_with_na_val = X_val_cleaned.columns[X_val_cleaned.isna().any()]\n",
    "cols_with_na_features_train = X_features_train_cleaned.columns[X_features_train_cleaned.isna().any()]\n",
    "all_cols_with_na = set(cols_with_na_train).union(set(cols_with_na_val)).union(set(cols_with_na_features_train))\n",
    "drop_cols = list(all_cols_with_na)\n",
    "X_train_cleaned = X_train_cleaned.drop(columns=drop_cols)\n",
    "X_val_cleaned = X_val_cleaned.drop(columns=drop_cols)\n",
    "X_features_train_cleaned = X_features_train_cleaned.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "c08e9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1_rollmean5</th>\n",
       "      <th>D2_rollmean5</th>\n",
       "      <th>D3_rollmean5</th>\n",
       "      <th>D4_rollmean5</th>\n",
       "      <th>D5_rollmean5</th>\n",
       "      <th>D6_rollmean5</th>\n",
       "      <th>D7_rollmean5</th>\n",
       "      <th>D8_rollmean5</th>\n",
       "      <th>D9_rollmean5</th>\n",
       "      <th>E1_rollmean5</th>\n",
       "      <th>...</th>\n",
       "      <th>V11_rollstd20</th>\n",
       "      <th>V12_rollstd20</th>\n",
       "      <th>V13_rollstd20</th>\n",
       "      <th>V2_rollstd20</th>\n",
       "      <th>V3_rollstd20</th>\n",
       "      <th>V4_rollstd20</th>\n",
       "      <th>V5_rollstd20</th>\n",
       "      <th>V6_rollstd20</th>\n",
       "      <th>V7_rollstd20</th>\n",
       "      <th>V8_rollstd20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.326470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205994</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.064361</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.326470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205994</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.064361</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.326470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205994</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.064361</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.326470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205994</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.064361</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.326470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205994</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.064361</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.720845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104215</td>\n",
       "      <td>0.214891</td>\n",
       "      <td>0.068393</td>\n",
       "      <td>0.057615</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.054971</td>\n",
       "      <td>0.523733</td>\n",
       "      <td>0.211907</td>\n",
       "      <td>0.107330</td>\n",
       "      <td>0.051271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.719789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109211</td>\n",
       "      <td>0.223626</td>\n",
       "      <td>0.066307</td>\n",
       "      <td>0.063395</td>\n",
       "      <td>0.137276</td>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.526851</td>\n",
       "      <td>0.200598</td>\n",
       "      <td>0.105251</td>\n",
       "      <td>0.052269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110043</td>\n",
       "      <td>0.222056</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.062737</td>\n",
       "      <td>0.142143</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>0.186592</td>\n",
       "      <td>0.105139</td>\n",
       "      <td>0.049157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6938</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111068</td>\n",
       "      <td>0.215146</td>\n",
       "      <td>0.069717</td>\n",
       "      <td>0.062666</td>\n",
       "      <td>0.142084</td>\n",
       "      <td>0.031799</td>\n",
       "      <td>0.485718</td>\n",
       "      <td>0.174361</td>\n",
       "      <td>0.109659</td>\n",
       "      <td>0.048913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.716625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112326</td>\n",
       "      <td>0.214111</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.062521</td>\n",
       "      <td>0.140258</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.511774</td>\n",
       "      <td>0.165228</td>\n",
       "      <td>0.118639</td>\n",
       "      <td>0.047438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6940 rows × 344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      D1_rollmean5  D2_rollmean5  D3_rollmean5  D4_rollmean5  D5_rollmean5  \\\n",
       "0              0.0           0.0           0.0           1.0           0.4   \n",
       "1              0.0           0.0           0.0           1.0           0.4   \n",
       "2              0.0           0.0           0.0           1.0           0.4   \n",
       "3              0.0           0.0           0.0           1.0           0.4   \n",
       "4              0.0           0.0           0.0           1.0           0.4   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "6935           0.0           0.0           0.0           0.0           0.4   \n",
       "6936           0.0           0.0           0.0           0.0           0.2   \n",
       "6937           0.0           0.0           0.2           0.0           0.0   \n",
       "6938           0.0           0.0           0.2           0.0           0.0   \n",
       "6939           0.0           0.0           0.2           0.0           0.0   \n",
       "\n",
       "      D6_rollmean5  D7_rollmean5  D8_rollmean5  D9_rollmean5  E1_rollmean5  \\\n",
       "0              0.0           0.0           0.0           0.6      2.326470   \n",
       "1              0.0           0.0           0.0           0.6      2.326470   \n",
       "2              0.0           0.0           0.0           0.6      2.326470   \n",
       "3              0.0           0.0           0.0           0.6      2.326470   \n",
       "4              0.0           0.0           0.0           0.6      2.326470   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "6935           0.0           0.2           0.0           0.6      0.720845   \n",
       "6936           0.0           0.2           0.0           0.4      0.719789   \n",
       "6937           0.0           0.2           0.0           0.2      0.718734   \n",
       "6938           0.0           0.0           0.0           0.0      0.717679   \n",
       "6939           0.0           0.0           0.0           0.0      0.716625   \n",
       "\n",
       "      ...  V11_rollstd20  V12_rollstd20  V13_rollstd20  V2_rollstd20  \\\n",
       "0     ...       0.000000       0.000000       0.205994      0.006483   \n",
       "1     ...       0.000000       0.000000       0.205994      0.006483   \n",
       "2     ...       0.000000       0.000000       0.205994      0.006483   \n",
       "3     ...       0.000000       0.000000       0.205994      0.006483   \n",
       "4     ...       0.000000       0.000000       0.205994      0.006483   \n",
       "...   ...            ...            ...            ...           ...   \n",
       "6935  ...       0.104215       0.214891       0.068393      0.057615   \n",
       "6936  ...       0.109211       0.223626       0.066307      0.063395   \n",
       "6937  ...       0.110043       0.222056       0.066681      0.062737   \n",
       "6938  ...       0.111068       0.215146       0.069717      0.062666   \n",
       "6939  ...       0.112326       0.214111       0.075046      0.062521   \n",
       "\n",
       "      V3_rollstd20  V4_rollstd20  V5_rollstd20  V6_rollstd20  V7_rollstd20  \\\n",
       "0         0.064361      0.006245      0.235007      0.000000      0.307308   \n",
       "1         0.064361      0.006245      0.235007      0.000000      0.307308   \n",
       "2         0.064361      0.006245      0.235007      0.000000      0.307308   \n",
       "3         0.064361      0.006245      0.235007      0.000000      0.307308   \n",
       "4         0.064361      0.006245      0.235007      0.000000      0.307308   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "6935      0.141045      0.054971      0.523733      0.211907      0.107330   \n",
       "6936      0.137276      0.047187      0.526851      0.200598      0.105251   \n",
       "6937      0.142143      0.042914      0.499615      0.186592      0.105139   \n",
       "6938      0.142084      0.031799      0.485718      0.174361      0.109659   \n",
       "6939      0.140258      0.025871      0.511774      0.165228      0.118639   \n",
       "\n",
       "      V8_rollstd20  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "...            ...  \n",
       "6935      0.051271  \n",
       "6936      0.052269  \n",
       "6937      0.049157  \n",
       "6938      0.048913  \n",
       "6939      0.047438  \n",
       "\n",
       "[6940 rows x 344 columns]"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "9ba852e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_tr, y_tr, X_va, y_va, *, k=30, top_corr=60,\n",
    "                     use_extratrees=True, val_last=252, n_repeats=5, seed=42, w_corr=0.6, w_perm=0.4):\n",
    "    # 1) keep a small validation slice\n",
    "    if val_last is not None and len(X_va) > val_last:\n",
    "        Xv = X_va.iloc[-val_last:].astype(np.float32).copy()\n",
    "        yv = y_va.iloc[-val_last:].to_numpy()\n",
    "    else:\n",
    "        Xv = X_va.astype(np.float32).copy()\n",
    "        yv = y_va.to_numpy()\n",
    "\n",
    "    # 2) univariate Pearson on TRAIN; take top N\n",
    "    corr_abs = X_tr.apply(lambda c: np.corrcoef(c, y_tr)[0,1], axis=0).abs().fillna(0.0)\n",
    "    cand = corr_abs.sort_values(ascending=False).head(min(top_corr, X_tr.shape[1])).index.tolist()\n",
    "\n",
    "    # 3) small, fast tree on TRAIN\n",
    "    Tree = ExtraTreesRegressor if use_extratrees else RandomForestRegressor\n",
    "    tree = Tree(\n",
    "        n_estimators=100, max_depth=8, min_samples_leaf=0.01, max_features=0.7,\n",
    "        n_jobs=-1, random_state=seed\n",
    "    ).fit(X_tr[cand].astype(np.float32), y_tr.to_numpy())\n",
    "\n",
    "    # 4) permutation importance on VALID (few repeats, parallel)\n",
    "    pi = permutation_importance(tree, Xv[cand], yv, n_repeats=n_repeats,\n",
    "                                random_state=seed, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "    perm = pd.Series(np.clip(pi.importances_mean, 0, None), index=cand)\n",
    "\n",
    "    # 5) combine by rank (robust to scaling)\n",
    "    r_corr = corr_abs.loc[cand].rank(ascending=False)\n",
    "    r_perm = perm.rank(ascending=False)\n",
    "    score = (w_corr * r_corr + w_perm * r_perm).sort_values(ascending=False)\n",
    "\n",
    "    selected = score.index[:min(k, len(score))].tolist()\n",
    "    summary = pd.DataFrame({\"corr_abs\": corr_abs.loc[cand], \"perm\": perm, \"score_rank\": score}).loc[score.index]\n",
    "    return selected, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "919c539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(y_true, y_pred, *, dropna: bool = True, margin: float = 0.0, count_ties: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Sign accuracy (hit rate): fraction of times sign(y_pred) == sign(y_true).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred : array-like\n",
    "        Equal-length sequences of numbers.\n",
    "    dropna : bool, default True\n",
    "        If True, drop any pair with NaN in either array.\n",
    "        If False and NaNs are present, returns np.nan.\n",
    "    margin : float, default 0.0\n",
    "        Treat predictions with |y_pred| <= margin as 0 (neutral band).\n",
    "    count_ties : bool, default False\n",
    "        If False, exclude any pair where sign is 0 on either side.\n",
    "        If True, include pairs with sign==0 and count them as correct\n",
    "        only when both are 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hit : float\n",
    "        Proportion in [0,1], or np.nan if no eligible pairs.\n",
    "    \"\"\"\n",
    "    a = np.asarray(y_true, dtype=float).flatten()\n",
    "    b = np.asarray(y_pred, dtype=float).flatten()\n",
    "\n",
    "    if a.shape != b.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "\n",
    "    mask = np.isfinite(a) & np.isfinite(b)\n",
    "    if not dropna and not mask.all():\n",
    "        return np.nan\n",
    "    a = a[mask]\n",
    "    b = b[mask]\n",
    "\n",
    "    # Apply neutral band to predictions\n",
    "    if margin > 0:\n",
    "        b = b.copy()\n",
    "        b[np.abs(b) <= margin] = 0.0\n",
    "\n",
    "    s_true = np.sign(a)\n",
    "    s_pred = np.sign(b)\n",
    "\n",
    "    if count_ties:\n",
    "        eligible = np.ones_like(s_true, dtype=bool)\n",
    "    else:\n",
    "        eligible = (s_true != 0) & (s_pred != 0)\n",
    "\n",
    "    if not np.any(eligible):\n",
    "        return np.nan\n",
    "\n",
    "    hits = (s_true[eligible] == s_pred[eligible]).mean()\n",
    "    return float(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "7b2d7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Pearson correlation coefficient between y_true and y_pred.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred : array-like\n",
    "        Equal-length sequences of numbers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corr : float\n",
    "        Pearson correlation coefficient in [-1,1], or np.nan if undefined.\n",
    "    \"\"\"\n",
    "    a = np.asarray(y_true, dtype=float).flatten()\n",
    "    b = np.asarray(y_pred, dtype=float).flatten()\n",
    "\n",
    "    if a.shape != b.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "\n",
    "    mask = np.isfinite(a) & np.isfinite(b)\n",
    "    a = a[mask]\n",
    "    b = b[mask]\n",
    "\n",
    "    if len(a) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    corr, _ = pearsonr(a, b)\n",
    "    return float(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "2dd66682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_train(X_train, y_train, features = None, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Train a Ridge regression model and evaluate on validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train : pd.Series\n",
    "        Training target.\n",
    "   \n",
    "    alpha : float, default 1.0\n",
    "        Regularization strength for Ridge regression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Ridge\n",
    "        Trained Ridge regression model.\n",
    "    \n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    print(features)\n",
    "\n",
    "    if features:\n",
    "        model.fit(X_train[features], y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "a2f0253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_train(X_train, y_train, features = None, C=1.0):\n",
    "    \"\"\"\n",
    "    Train a Logistic regression model and evaluate on validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train : pd.Series\n",
    "        Training target.\n",
    "   \n",
    "    C : float, default 1.0\n",
    "        Inverse of regularization strength for Logistic regression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : LogisticRegression\n",
    "        Trained Logistic regression model.\n",
    "    \n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logistic', LogisticRegression(C=C, max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    if features:\n",
    "        model.fit(X_train[features], y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "834c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trees_train(X_train, y_train, features = None, type='RandomForest'):\n",
    "    \"\"\"\n",
    "    Train a Tree Regressor and evaluate on validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train : pd.Series\n",
    "        Training target.\n",
    "   \n",
    "    n_estimators : int, default 100\n",
    "        Number of trees in the forest.\n",
    "    \n",
    "    max_depth : int or None, default None\n",
    "        Maximum depth of the tree.\n",
    "\n",
    "    random_state : int, default 42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Regressor\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if type == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100,\n",
    "            max_depth=8,\n",
    "            min_samples_leaf=0.01,     # 1% of samples per leaf (robust)\n",
    "            min_samples_split=0.02,\n",
    "            max_features=0.7,\n",
    "            bootstrap=True,\n",
    "            n_jobs=-1, random_state=42)\n",
    "        \n",
    "    elif type == 'ExtraTrees':\n",
    "        model = ExtraTreesRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            min_samples_leaf=0.01,\n",
    "            min_samples_split=0.02,\n",
    "            max_features=0.7,\n",
    "            bootstrap=False,\n",
    "            n_jobs=-1, random_state=42\n",
    "        )\n",
    "\n",
    "    elif type == 'XGBoost':\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.10,\n",
    "            max_depth=4,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            min_child_weight=10,       # combats noise\n",
    "            reg_lambda=2.0,\n",
    "            objective=\"reg:squarederror\",\n",
    "            n_jobs=-1, random_state=42\n",
    "        )\n",
    "    elif type == 'LightGBM':\n",
    "        model = LGBMRegressor(\n",
    "            verbosity = -1,\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.10,\n",
    "            max_depth=6,\n",
    "            num_leaves=31,             # <= 2^max_depth for safety\n",
    "            min_data_in_leaf=100,      # robust on small-signal data\n",
    "            feature_fraction=0.7,\n",
    "            bagging_fraction=0.7,\n",
    "            bagging_freq=1,\n",
    "            lambda_l2=5.0,\n",
    "            extra_trees=True,          # adds randomness like ExtraTrees\n",
    "            n_jobs=-1, random_state=42\n",
    "        )\n",
    "    if features:\n",
    "        model.fit(X_train[features], y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "a992ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_calibrate(pred_train, y_train, *, dropna: bool = True, fit_intercept: bool = True):\n",
    "    \"\"\"\n",
    "    Fit a scikit-learn LinearRegression on TRAIN predictions:\n",
    "        y = alpha + beta * pred\n",
    "\n",
    "    Returns a Pipeline that reshapes 1D inputs and applies the fitted LinearRegression.\n",
    "    You can call .predict(...) on it with a 1D array/list/Series of predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_train : array-like, shape (n_samples,)\n",
    "        Model predictions on the training window (e.g., your tree's outputs).\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "        Realized targets on the training window (e.g., next-day returns).\n",
    "    dropna : bool, default True\n",
    "        Drop pairs with NaN/Inf before fitting. If False and NaNs exist, raises ValueError.\n",
    "    fit_intercept : bool, default True\n",
    "        Passed to LinearRegression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : sklearn Pipeline\n",
    "        Use model.predict(new_pred) to get calibrated predictions.\n",
    "        Access alpha/beta via:\n",
    "            alpha = model.named_steps['lr'].intercept_\n",
    "            beta  = model.named_steps['lr'].coef_[0]\n",
    "    \"\"\"\n",
    "    x = np.asarray(pred_train, dtype=float).flatten()\n",
    "    y = np.asarray(y_train, dtype=float).flatten()\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(\"pred_train and y_train must have the same shape\")\n",
    "\n",
    "    mask = np.isfinite(x) & np.isfinite(y)\n",
    "    if not dropna and not mask.all():\n",
    "        raise ValueError(\"NaNs/Infs present; set dropna=True to filter them out.\")\n",
    "    x, y = x[mask], y[mask]\n",
    "    if x.size < 2:\n",
    "        raise ValueError(\"Not enough data to calibrate (need ≥2 finite pairs).\")\n",
    "\n",
    "    # Pipeline so you can pass 1D arrays to .predict() without manual reshape\n",
    "    model = Pipeline(steps=[\n",
    "        (\"reshape\", FunctionTransformer(lambda z: np.asarray(z, dtype=float).reshape(-1, 1), validate=False)),\n",
    "        (\"lr\", LinearRegression(fit_intercept=fit_intercept)),\n",
    "    ])\n",
    "    model.fit(x, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "0e6ffbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, calibrate_model):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Trained model\n",
    "        The trained regression model.\n",
    "    X : pd.DataFrame\n",
    "        Features for prediction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    calibrated_pred = calibrate_model.predict(y_pred)\n",
    "    return calibrated_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "49e0a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_features, regression_feature_summary = feature_selection(X_train_cleaned, y_train, X_features_train_cleaned, y_features_train, w_corr=0.9, w_perm=0.1, k = 30, top_corr=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "e4114e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D4_rollmean20', 'V13_rollmean20', 'D6_rollstd5', 'V5_rollstd20', 'D9_rollstd5', 'I8_rollstd20', 'D4_rollmean5', 'P10_rollstd5', 'I2_rollmean20', 'E8_rollstd5', 'P8_rollstd5', 'M17_rollmean5', 'M17_rollmean20', 'S2_rollmean20', 'I8_rollstd5', 'P7_rollmean5', 'I2_rollmean5', 'D9_rollmean5', 'D6_rollmean5', 'D7_rollstd5', 'D7_rollmean5', 'E12_rollmean5', 'V13_rollmean5', 'E12_rollmean20', 'S2_rollmean5', 'E11_rollmean5', 'E11_rollmean20', 'M4_rollmean5', 'S5_rollmean20', 'S5_rollmean5']\n",
      "Ridge Hit Rate: 0.5139043381535039 Pearson Correlation: 0.026745103772223253\n"
     ]
    }
   ],
   "source": [
    "ridge = ridge_train(X_train_cleaned, y_train, features = regression_features, alpha=1.0)\n",
    "ridge_predict_train = ridge.predict(X_train_cleaned[regression_features])\n",
    "calibrate_model = linear_calibrate(ridge_predict_train, y_train)\n",
    "ridge_predict_val = predict(ridge, X_val_cleaned[regression_features], calibrate_model)\n",
    "ridge_hit_rate = hit_rate(y_val, ridge_predict_val)\n",
    "ridge_pearson = pearson_corr(y_val, ridge_predict_val)\n",
    "print(\"Ridge Hit Rate:\", ridge_hit_rate, \"Pearson Correlation:\", ridge_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "5866c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_y = [1 if x > 0 else 0 for x in list(np.asarray(y_train).flatten())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "14f89d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Hit Rate: 0.5350389321468298 Pearson Correlation: 0.043807784585532215\n"
     ]
    }
   ],
   "source": [
    "logistic = logistic_train(X_train_cleaned, pd.Series(logistic_y), features = regression_features, C=1.0)\n",
    "logistic_predict_train = logistic.predict_proba(X_train_cleaned[regression_features])\n",
    "logistic_calibrate_model = linear_calibrate(logistic_predict_train[:,1], y_train)\n",
    "logistic_predict_val = predict(logistic, X_val_cleaned[regression_features], logistic_calibrate_model)\n",
    "logistic_hit_rate = hit_rate(y_val, logistic_predict_val)\n",
    "logistic_pearson = pearson_corr(y_val, logistic_predict_val)\n",
    "print(\"Logistic Hit Rate:\", logistic_hit_rate, \"Pearson Correlation:\", logistic_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "0f41e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "btree_features, btree_feature_summary = feature_selection(X_train_cleaned, y_train, X_features_train_cleaned, y_features_train, w_corr=0.7, w_perm=0.3, k = 200, top_corr=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "ff164609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Hit Rate: 0.5255839822024472 Pearson Correlation: 0.047444636426307404\n"
     ]
    }
   ],
   "source": [
    "lightgbm = trees_train(X_train_cleaned, y_train, features=btree_features, type='LightGBM')\n",
    "lightgbm_predict_train = lightgbm.predict(X_train_cleaned[btree_features])\n",
    "lightgbm_calibrate_model = linear_calibrate(lightgbm_predict_train, y_train)\n",
    "lightgbm_predict_val = predict(lightgbm, X_val_cleaned[btree_features], lightgbm_calibrate_model)\n",
    "lightgbm_hit_rate = hit_rate(y_val, lightgbm_predict_val)\n",
    "lightgbm_pearson = pearson_corr(y_val, lightgbm_predict_val)\n",
    "print(\"LightGBM Hit Rate:\", lightgbm_hit_rate, \"Pearson Correlation:\", lightgbm_pearson)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "e1de1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_features, tree_feature_summary = feature_selection(X_train_cleaned, y_train, X_features_train_cleaned, y_features_train, w_corr=0.7, w_perm=0.3, k = 30, top_corr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "33f94925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Hit Rate: 0.4949944382647386 Pearson Correlation: 0.05187643236790405\n"
     ]
    }
   ],
   "source": [
    "rf = trees_train(X_train_cleaned, y_train, features=tree_features, type='RandomForest')\n",
    "rf_predict_train = rf.predict(X_train_cleaned[tree_features])\n",
    "rf_calibrate_model = linear_calibrate(rf_predict_train, y_train)        \n",
    "rf_predict_val = predict(rf, X_val_cleaned[tree_features], rf_calibrate_model)\n",
    "rf_hit_rate = hit_rate(y_val, rf_predict_val)\n",
    "rf_pearson = pearson_corr(y_val, rf_predict_val)\n",
    "print(\"Random Forest Hit Rate:\", rf_hit_rate, \"Pearson Correlation:\", rf_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "9ad42c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Hit Rate: 0.5367074527252503 Pearson Correlation: 0.058462083308470844\n"
     ]
    }
   ],
   "source": [
    "ensemble_val = (0.3 * logistic_predict_val + 0.4 * lightgbm_predict_val + 0.3 * rf_predict_val)\n",
    "essemble_hit_rate = hit_rate(y_val, ensemble_val)\n",
    "ensemble_pearson = pearson_corr(y_val, ensemble_val)\n",
    "print(\"Ensemble Hit Rate:\", essemble_hit_rate, \"Pearson Correlation:\", ensemble_pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "919d9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_var(lam, r, v0):\n",
    "    v = np.empty_like(r)\n",
    "    v[0] = v0\n",
    "    for t in range(1, r.size):\n",
    "        v[t] = lam*v[t-1] + (1-lam)*r[t-1]**2\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "e55d402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ewma_variance(r_train,\n",
    "                        lam_grid = None,\n",
    "                        criterion = \"qlike\",\n",
    "                        df_t: int = 8):\n",
    "    \"\"\"\n",
    "    Choose EWMA lambda on TRAIN (no leakage).\n",
    "    Returns a dict {\"lam\": ..., \"init_var\": ...}.\n",
    "    \"\"\"\n",
    "    r = np.asarray(r_train, float).reshape(-1)\n",
    "    if lam_grid is None:\n",
    "        lam_grid = np.linspace(0.90, 0.995, num = 10)\n",
    "\n",
    "    v0 = np.var(r[:max(50, len(r)//5)]) if r.size else 0.0\n",
    "\n",
    "    best = (np.inf, lam_grid[0])\n",
    "    m0 = max(20, int(0.1*len(r)))  # drop warmup in scoring\n",
    "    i = 0\n",
    "    for lam in lam_grid:\n",
    "        v = ewma_var(lam, r, v0)\n",
    "        rr, vv = r[m0:], v[m0:]\n",
    "        if criterion in (\"qlike\", \"nll_norm\"):\n",
    "            loss = np.mean(np.log(vv) + (rr**2)/vv)\n",
    "        elif criterion == \"nll_t\":\n",
    "            nu = float(df_t)\n",
    "            loss = np.mean(np.log(vv) + (nu+1)*np.log1p((rr**2)/(nu*vv)))\n",
    "        elif criterion == \"mse\":\n",
    "            loss = np.mean((rr**2 - vv)**2)\n",
    "        else:\n",
    "            raise ValueError(\"criterion must be one of {'qlike','nll_norm','nll_t','mse'}\")\n",
    "        if loss < best[0]:\n",
    "            best = (loss, lam)\n",
    "        i += 1\n",
    "\n",
    "    return {\"lam\": float(best[1]), \"init_var\": float(v0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "6beed28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_garch11(r_train,\n",
    "                  dist = \"t\"):\n",
    "    \"\"\"\n",
    "    Fit ConstantMean + GARCH(1,1) on TRAIN. Returns a dict with fitted params & dist.\n",
    "    (We store the fitted result to reuse its parameters in validation forecasts.)\n",
    "    \"\"\"\n",
    "    r = np.asarray(r_train, float).reshape(-1)\n",
    "    am = ConstantMean(r * 100.0)              # scale to percent for stability\n",
    "    am.volatility = GARCH(1, 0, 1)\n",
    "    am.distribution = StudentsT() if dist == \"t\" else Normal()\n",
    "    res = am.fit(disp=\"off\")\n",
    "\n",
    "    model = {\n",
    "        \"dist\": dist,\n",
    "        \"params\": res.params,                 # pandas Series of parameters\n",
    "        \"scale\": 100.0\n",
    "    }\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "5d1c89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree_variance_qlike_surrogate(X_train,\n",
    "                                        r_train,\n",
    "                                        *,\n",
    "                                        n_estimators: int = 300,\n",
    "                                        max_depth: int = 8,\n",
    "                                        random_state: int = 42):\n",
    "    \"\"\"\n",
    "    QLIKE surrogate: predict s = log(r^2 + eps) on TRAIN, then variance = exp(ŝ).\n",
    "    (Minimizing E[(s - log r^2)^2] targets the QLIKE optimum since QLIKE(s)=s + r^2 e^{-s}.)\n",
    "    Returns fitted ExtraTrees model plus epsilon used.\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    y = np.log(np.asarray(r_train, float).reshape(-1)**2 + eps)\n",
    "    X = np.asarray(X_train, float)\n",
    "    model = ExtraTreesRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=max(1, int(0.01*len(y))),\n",
    "        max_features=0.7,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state\n",
    "    ).fit(X, y)\n",
    "    return {\"model\": model, \"eps\": eps}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "d6a22034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ewma_on_valid(train_model: dict,\n",
    "                        r_train,\n",
    "                        r_valid):\n",
    "    \"\"\"\n",
    "    Walk-forward EWMA variance for VALID using trained {\"lam\",\"init_var\"}.\n",
    "    Returns pd.Series aligned to r_valid.index if available.\n",
    "    \"\"\"\n",
    "    lam = float(train_model[\"lam\"]); v0 = float(train_model[\"init_var\"])\n",
    "    r_tr = np.asarray(r_train, float).reshape(-1)\n",
    "    r_va = np.asarray(r_valid, float).reshape(-1)\n",
    "    idx_va = getattr(r_valid, \"index\", pd.RangeIndex(len(r_va)))\n",
    "\n",
    "    r_all = np.concatenate([r_tr, r_va])\n",
    "    v = np.empty_like(r_all)\n",
    "    v[0] = v0\n",
    "    for t in range(1, len(r_all)):\n",
    "        v[t] = lam*v[t-1] + (1-lam)*r_all[t-1]**2\n",
    "    return pd.Series(v[len(r_tr):], index=idx_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "a35722e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_garch_on_valid(train_model: dict,\n",
    "                         r_train,\n",
    "                         r_valid,\n",
    "                         refit_every = 21):\n",
    "    \"\"\"\n",
    "    Walk-forward GARCH(1,1) variance on VALID.\n",
    "    - Start from TRAIN; if refit_every is None, use fixed parameters (one-shot fit).\n",
    "    - If refit_every is int, refit every N days using expanding history (adaptive, no leakage).\n",
    "    Returns pd.Series aligned to r_valid.index if available.\n",
    "    \"\"\"\n",
    "    dist = train_model.get(\"dist\", \"t\")\n",
    "    scale = float(train_model.get(\"scale\", 100.0))\n",
    "    r_tr = np.asarray(r_train, float).reshape(-1)\n",
    "    r_va = np.asarray(r_valid, float).reshape(-1)\n",
    "    idx_va = getattr(r_valid, \"index\", pd.RangeIndex(len(r_va)))\n",
    "\n",
    "    var_va = np.empty_like(r_va)\n",
    "    i = 0\n",
    "    hist = r_tr.copy()\n",
    "\n",
    "    # If no refits: fit once and forecast the entire validation\n",
    "    if refit_every is None:\n",
    "        am = ConstantMean(hist * scale)\n",
    "        am.volatility = GARCH(1, 0, 1)\n",
    "        am.distribution = StudentsT() if dist == \"t\" else Normal()\n",
    "        res = am.fit(disp=\"off\")\n",
    "        fc = res.forecast(horizon=len(r_va), reindex=False)\n",
    "        var_va[:] = fc.variance.values[-1, :len(r_va)] / (scale**2)\n",
    "        return pd.Series(var_va, index=idx_va)\n",
    "\n",
    "    # Periodic refits\n",
    "    step = max(1, int(refit_every))\n",
    "    while i < len(r_va):\n",
    "        h = min(step, len(r_va) - i)\n",
    "        am = ConstantMean(hist * scale)\n",
    "        am.volatility = GARCH(1, 0, 1)\n",
    "        am.distribution = StudentsT() if dist == \"t\" else Normal()\n",
    "        res = am.fit(disp=\"off\")\n",
    "        fc = res.forecast(horizon=h, reindex=False)\n",
    "        var_va[i:i+h] = fc.variance.values[-1, :h] / (scale**2)\n",
    "        # append realized returns for next refit (expanding window)\n",
    "        hist = np.r_[hist, r_va[i:i+h]]\n",
    "        i += h\n",
    "\n",
    "    return pd.Series(var_va, index=idx_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "fd453d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tree_on_valid(train_model: dict,\n",
    "                        X_valid):\n",
    "    \"\"\"\n",
    "    Predict VALID variance from ExtraTrees QLIKE surrogate.\n",
    "    Returns pd.Series (variance), preserves X_valid index if DataFrame.\n",
    "    \"\"\"\n",
    "    model = train_model[\"model\"]; eps = float(train_model[\"eps\"])\n",
    "    Xv = np.asarray(X_valid, float)\n",
    "    s_hat = model.predict(Xv)                # ŝ ≈ E[log r^2 | X]\n",
    "    var_hat = np.exp(s_hat)                  # v̂ = exp(ŝ)  (>=0)\n",
    "    idx = getattr(X_valid, \"index\", pd.RangeIndex(len(var_hat)))\n",
    "    return pd.Series(var_hat, index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "03cac4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike(r, vhat, eps=1e-12):\n",
    "    r = np.asarray(r, float).reshape(-1)\n",
    "    v = np.asarray(vhat, float).reshape(-1)\n",
    "    v = np.clip(v, eps, None)\n",
    "    return float(np.mean(np.log(v) + (r**2)/v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "23ea93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict_train = predict(rf, X_train_cleaned[tree_features], rf_calibrate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "40533897",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_predict_train = predict(lightgbm, X_train_cleaned[btree_features], lightgbm_calibrate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "83696634",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_predict_train = predict(logistic, X_train_cleaned[features_selected], logistic_calibrate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "4940e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_train = (0.3 * rf_predict_train + 0.4 * lightgbm_predict_train + 0.3 * logistic_predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "11fc4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = y_train - ensemble_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "4c1d180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ewma_model = train_ewma_variance(X_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "5df309fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ewma_var = apply_ewma_on_valid(ewma_model, y_train, ensemble_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "a55db5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.38421732464044"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlike(y_val, ewma_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "fe55d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model = train_garch11(X_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "274f977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_var = apply_garch_on_valid(garch_model, y_train, ensemble_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "2da5f697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.127822114224793"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlike(y_val, garch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "898a722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = train_tree_variance_qlike_surrogate(X_train_cleaned, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "4c74522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_var = apply_tree_on_valid(tree_model, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "bcf8ad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.646020158798007"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlike(y_val, tree_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
